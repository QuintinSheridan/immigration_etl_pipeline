{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Us Weather and Immigragration Study\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project an ETL Pipeline is created to to combine data from 4 sifferent data sets: immigration, temperature, demographics, and airports to asses immigration paterns in the US.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from io import StringIO # python3; python2: BytesIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get environment variables for connecting to AWS\n",
    "#AWS_SECRET = os.environ['AWS_SECRET']\n",
    "#AWS_KEY = os.environ['AWS_KEY']\n",
    "#DB_USER = os.environ['DB_USER']\n",
    "#DB_PASSWORD = os.environ['DB_PASSWORD']\n",
    "#ARN = os.environ['ARN']\n",
    "\n",
    "AWS_SECRET = 'AKIAXBEQDW5I6X2BDYF7'\n",
    "AWS_KEY = 'q4JwS2TEUaznc1+uLjcUFPR/XhsUrGchhZedCYEI'\n",
    "DB = 'dev'\n",
    "DB_USER = 'dwhuser'\n",
    "DB_PASSWORD = 'dwhBub42'\n",
    "HOST = 'redshift-cluster-1.ca7m8qui9aaj.us-west-2.redshift.amazonaws.com'\n",
    "ARN='arn:aws:iam::483486054225:role/dwhRole'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, immigration data sets are combined with airport and demographic data sets to evaluate flight patternd of immigrants into the United States.  The data is used in aggregation to evaluate the total number of immigrant visits by airport type, airport, state, and the majority ethnicity of the city.  First, the data sets are explored, cleaned, and uploaded to s3 storage.  Then the storage table are coppied to staging tables in Amazon Redshift.  Final, the staging tables are queried to make fact and dimension tables as well as tables with aggregate values.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "The following datasets were used for this project:\n",
    "\n",
    "I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A single month of United states immigration data was used.  The data set contains imformation on immigration suych as information on the immigrants, the immigration status, and the trip information.  The data sets can be found here:\n",
    "link - https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "World Temperature Data: This dataset came from Kaggle. It contains date, average temperature, temperature error, city, Country, lattitude, and longitude\n",
    "link - https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "\n",
    "US City Demograhphic Data: This data comes from OpenSoft.  This data contains demogrpahic imformation for US cities.\n",
    "link - https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "Airport Code Table: This dataset comes from datahub. This is a simple table of airport codes and corresponding cities.\n",
    "link - https://datahub.io/core/airport-codes#data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Temperature Data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "pd_temp_df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Imigration Data\n",
    "sprk_im_df = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Demographic Data\n",
    "pd_demographic_df = pd.read_csv('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in Airport Data\n",
    "pd_airport_df = pd.read_csv('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper function to write dfs to S3 storage for staging\n",
    "def write_to_staging(df, bucket, csv_name):\n",
    "    \"\"\"Function that writes a df to csv in S3\n",
    "    Args:\n",
    "        df (dataframe): pandas dataframe\n",
    "        bucket (str): S3 bucket name\n",
    "        csv_name (str): name for output csv\n",
    "    \"\"\"\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3',\n",
    "                                aws_access_key_id=AWS_SECRET, \n",
    "                                aws_secret_access_key=AWS_KEY, \n",
    "                                region_name='us-west-2')\n",
    "    s3_resource.Object(bucket, csv_name).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# countries = pd_temp_df['Country'].unique()\n",
    "# countries.sort()\n",
    "# countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temp_df = pd_temp_df[pd_temp_df['Country']=='United States']\n",
    "pd_temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove row swith null values\n",
    "pd_temp_df = pd_temp_df.dropna()\n",
    "pd_temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# format lat and lon\n",
    "pd_temp_df['Latitude'] = pd_temp_df['Latitude'].str[:-1] \n",
    "pd_temp_df['Longitude'] = '-' + pd_temp_df['Longitude'].str[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert from metric to english units\n",
    "pd_temp_df['AverageTemperature'] = pd_temp_df['AverageTemperature'].apply(lambda x: (1.8*x)+32) \n",
    "pd_temp_df['AverageTemperatureUncertainty'] = pd_temp_df['AverageTemperatureUncertainty'].apply(lambda x: (1.8*x)+32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove country\n",
    "pd_temp_df = pd_temp_df.drop('Country', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_data_dict = {\n",
    "                          'dt': 'date' , \n",
    "                          'AverageTemperature': 'average temperature in Ferenheit' , \n",
    "                          'AverageTemperatureUncertainty': 'temperature uncertainty in Ferenheit', \n",
    "                          'City': 'City', \n",
    "                          'Latitude': 'latitude', \n",
    "                          'Longitude': 'longitude'\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_temp_df['dt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### We can see that the temperature data only goes to 2013, but the immigration data is from 2016 so we will not use the temperature data in future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Imigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#for col in sprk_im_df.limit(1).toPandas().columns:\n",
    "#    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a table view to query gainst\n",
    "sprk_im_df.createOrReplaceTempView(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we do not need to keep i94yr as it has a single value\n",
    "years = spark.sql('''\n",
    "    SELECT \n",
    "        DISTINCT i94yr\n",
    "    FROM\n",
    "        immigration\n",
    "''')\n",
    "years.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we do not need to keep i94month as it has a single value\n",
    "months = spark.sql('''\n",
    "    SELECT \n",
    "        DISTINCT i94mon\n",
    "    FROM\n",
    "        immigration\n",
    "''')\n",
    "months.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94visa = spark.sql('''\n",
    "    SELECT \n",
    "        DISTINCT i94visa\n",
    "    FROM\n",
    "        immigration\n",
    "''')\n",
    "i94visa.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visatype = spark.sql('''\n",
    "#     SELECT \n",
    "#         DISTINCT visatype\n",
    "#     FROM\n",
    "#         immigration\n",
    "# ''')\n",
    "# visatype.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a data dict for kept columns and use to remove unneccesary columns\n",
    "immigration_data_dict = {'i94cit': 'city code' , \n",
    "                          'i94port': 'port code' , \n",
    "                          'i94mode':  'transportation code', \n",
    "                          'arrdate': 'arrival date',\n",
    "                          'i94res': 'country code for immigrant', \n",
    "                          'i94addr': 'state code', \n",
    "                          'depDate': 'departure date',\n",
    "                          'i94bir': 'immigrant age', \n",
    "                          'i94visa': 'visa type',\n",
    "                          'gender': 'gender'}\n",
    "\n",
    "imigration_keep_cols = list(immigration_data_dict.keys())\n",
    "\n",
    "sprk_im_df = sprk_im_df.select(imigration_keep_cols)\n",
    "\n",
    "sprk_im_df.limit(3).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_im_df = sprk_im_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_im_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows with null values\n",
    "pd_im_df = pd_im_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_im_df['i94port'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop any duplicate rows\n",
    "pd_im_df = pd_im_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write the cleaned imigration to csv in staging\n",
    "write_to_staging(pd_im_df, 'qscapstone', 'immigration.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>07FA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ocean Reef Club Airport</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Key Largo</td>\n",
       "      <td>07FA</td>\n",
       "      <td>OCA</td>\n",
       "      <td>07FA</td>\n",
       "      <td>-80.274803161621, 25.325399398804</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Pilot Station Airport</td>\n",
       "      <td>305.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Pilot Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PQS</td>\n",
       "      <td>0AK</td>\n",
       "      <td>-162.899994, 61.934601</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ident           type                     name  elevation_ft continent  \\\n",
       "440  07FA  small_airport  Ocean Reef Club Airport           8.0       NaN   \n",
       "594   0AK  small_airport    Pilot Station Airport         305.0       NaN   \n",
       "\n",
       "    iso_country iso_region   municipality gps_code iata_code local_code  \\\n",
       "440          US      US-FL      Key Largo     07FA       OCA       07FA   \n",
       "594          US      US-AK  Pilot Station      NaN       PQS        0AK   \n",
       "\n",
       "                           coordinates state  \n",
       "440  -80.274803161621, 25.325399398804    FL  \n",
       "594             -162.899994, 61.934601    AK  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_airport_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# parse state out of iso_region\n",
    "pd_airport_df['state'] = pd_airport_df['iso_region'].str[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_airport_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pd_airport_df['iso_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport_df = pd_airport_df[(pd_airport_df['iso_country']=='US') & (pd_airport_df['iata_code'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2019, 13)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_airport_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>07FA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ocean Reef Club Airport</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Key Largo</td>\n",
       "      <td>07FA</td>\n",
       "      <td>OCA</td>\n",
       "      <td>07FA</td>\n",
       "      <td>-80.274803161621, 25.325399398804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Pilot Station Airport</td>\n",
       "      <td>305.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Pilot Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PQS</td>\n",
       "      <td>0AK</td>\n",
       "      <td>-162.899994, 61.934601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ident           type                     name  elevation_ft continent  \\\n",
       "440  07FA  small_airport  Ocean Reef Club Airport           8.0       NaN   \n",
       "594   0AK  small_airport    Pilot Station Airport         305.0       NaN   \n",
       "\n",
       "    iso_country iso_region   municipality gps_code iata_code local_code  \\\n",
       "440          US      US-FL      Key Largo     07FA       OCA       07FA   \n",
       "594          US      US-AK  Pilot Station      NaN       PQS        0AK   \n",
       "\n",
       "                           coordinates  \n",
       "440  -80.274803161621, 25.325399398804  \n",
       "594             -162.899994, 61.934601  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_airport_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport_df['same_id'] = pd_airport_df['ident'] == pd_airport_df['local_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_airport_df['same_id'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>state</th>\n",
       "      <th>same_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>2IG4</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ed-Air Airport</td>\n",
       "      <td>426.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IN</td>\n",
       "      <td>Oaktown</td>\n",
       "      <td>I20</td>\n",
       "      <td>OTN</td>\n",
       "      <td>I20</td>\n",
       "      <td>-87.4997024536, 38.851398468</td>\n",
       "      <td>IN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>2Z1</td>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>Entrance Island Seaplane Base</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Entrance Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-133.43848, 57.412201</td>\n",
       "      <td>AK</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10470</th>\n",
       "      <td>AHT</td>\n",
       "      <td>closed</td>\n",
       "      <td>Amchitka Army Airfield</td>\n",
       "      <td>215.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Amchitka Island</td>\n",
       "      <td>PAHT</td>\n",
       "      <td>AHT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.259166667, 51.3777777778</td>\n",
       "      <td>AK</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>ARX</td>\n",
       "      <td>closed</td>\n",
       "      <td>Asbury Park Neptune Air Terminal</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NJ</td>\n",
       "      <td>Asbury Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-74.0908333333, 40.2193055556</td>\n",
       "      <td>NJ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>AUS</td>\n",
       "      <td>closed</td>\n",
       "      <td>Austin Robert Mueller Municipal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KAUS</td>\n",
       "      <td>AUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-97.6997852325, 30.2987223546</td>\n",
       "      <td>TX</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ident           type                              name  elevation_ft  \\\n",
       "3143   2IG4  small_airport                    Ed-Air Airport         426.0   \n",
       "3643    2Z1  seaplane_base     Entrance Island Seaplane Base           0.0   \n",
       "10470   AHT         closed            Amchitka Army Airfield         215.0   \n",
       "11498   ARX         closed  Asbury Park Neptune Air Terminal          95.0   \n",
       "11676   AUS         closed   Austin Robert Mueller Municipal           NaN   \n",
       "\n",
       "      continent iso_country iso_region     municipality gps_code iata_code  \\\n",
       "3143        NaN          US      US-IN          Oaktown      I20       OTN   \n",
       "3643        NaN          US      US-AK  Entrance Island      NaN       HBH   \n",
       "10470       NaN          US      US-AK  Amchitka Island     PAHT       AHT   \n",
       "11498       NaN          US      US-NJ      Asbury Park      NaN       ARX   \n",
       "11676       NaN          US      US-TX              NaN     KAUS       AUS   \n",
       "\n",
       "      local_code                    coordinates state  same_id  \n",
       "3143         I20   -87.4997024536, 38.851398468    IN    False  \n",
       "3643         NaN          -133.43848, 57.412201    AK    False  \n",
       "10470        NaN   179.259166667, 51.3777777778    AK    False  \n",
       "11498        NaN  -74.0908333333, 40.2193055556    NJ    False  \n",
       "11676        NaN  -97.6997852325, 30.2987223546    TX    False  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_airport_df[pd_airport_df['same_id']==False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_im_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-017ac8d3588e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim_ports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_im_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i94port'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mim_ports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mim_ports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_im_df' is not defined"
     ]
    }
   ],
   "source": [
    "im_ports = pd_im_df['i94port'].unique()\n",
    "im_ports.sort()\n",
    "im_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iata_codes = pd_airport_df['iata_code'].unique()\n",
    "iata_codes.sort()\n",
    "iata_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we can see that we can join the immigration data to the airport data on pd_im_df.i94port = pd_airport_df.iata_code\n",
    "len(list(set(iata_codes).intersection(set(im_ports))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_airport_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split coordinates int lattitude and longitude\n",
    "pd_airport_df['latitude'] = pd_airport_df['coordinates'].str.split(',').str[1]\n",
    "pd_airport_df['longitude'] = pd_airport_df['coordinates'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>state</th>\n",
       "      <th>municipality</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>07FA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Ocean Reef Club Airport</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>FL</td>\n",
       "      <td>Key Largo</td>\n",
       "      <td>OCA</td>\n",
       "      <td>25.325399398804</td>\n",
       "      <td>-80.274803161621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Pilot Station Airport</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>AK</td>\n",
       "      <td>Pilot Station</td>\n",
       "      <td>PQS</td>\n",
       "      <td>61.934601</td>\n",
       "      <td>-162.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0CO2</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Crested Butte Airpark</td>\n",
       "      <td>US-CO</td>\n",
       "      <td>CO</td>\n",
       "      <td>Crested Butte</td>\n",
       "      <td>CSE</td>\n",
       "      <td>38.851918</td>\n",
       "      <td>-106.928341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>0TE7</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>LBJ Ranch Airport</td>\n",
       "      <td>US-TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>Johnson City</td>\n",
       "      <td>JCY</td>\n",
       "      <td>30.251800537100003</td>\n",
       "      <td>-98.62249755859999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>13MA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Metropolitan Airport</td>\n",
       "      <td>US-MA</td>\n",
       "      <td>MA</td>\n",
       "      <td>Palmer</td>\n",
       "      <td>PMX</td>\n",
       "      <td>42.223300933800004</td>\n",
       "      <td>-72.31140136719999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ident           type                     name iso_region state  \\\n",
       "440   07FA  small_airport  Ocean Reef Club Airport      US-FL    FL   \n",
       "594    0AK  small_airport    Pilot Station Airport      US-AK    AK   \n",
       "673   0CO2  small_airport    Crested Butte Airpark      US-CO    CO   \n",
       "1088  0TE7  small_airport        LBJ Ranch Airport      US-TX    TX   \n",
       "1402  13MA  small_airport     Metropolitan Airport      US-MA    MA   \n",
       "\n",
       "       municipality iata_code             latitude           longitude  \n",
       "440       Key Largo       OCA      25.325399398804    -80.274803161621  \n",
       "594   Pilot Station       PQS            61.934601         -162.899994  \n",
       "673   Crested Butte       CSE            38.851918         -106.928341  \n",
       "1088   Johnson City       JCY   30.251800537100003  -98.62249755859999  \n",
       "1402         Palmer       PMX   42.223300933800004  -72.31140136719999  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unneccessary columns\n",
    "air_data_dict = {\n",
    "    'ident': 'pkey dentifier', \n",
    "    'type': 'type of airport', \n",
    "    'name': 'airport name', \n",
    "    'iso_region': 'airport region', \n",
    "    'state': 'state',\n",
    "    'municipality': 'airport municipality', \n",
    "    'iata_code': 'code for airport, maps to i94port in imigration',\n",
    "    'latitude': 'latitude',\n",
    "    'longitude': 'longitude'  \n",
    "}\n",
    "\n",
    "air_keep_cols = list(air_data_dict.keys())\n",
    "pd_airport_df = pd_airport_df[air_keep_cols]\n",
    "pd_airport_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write airports to s3 for staging\n",
    "pd_airport_df = pd_airport_df.drop_duplicates()\n",
    "write_to_staging(pd_airport_df, 'qscapstone', 'airports.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Demographic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd_demographic_df.head(1)\n",
    "# heare we can see that we will need to perform some string parsing \n",
    "# to get this dataset into a usable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# parse out the table columns and reformat certain characters\n",
    "col_string = pd_demographic_df.columns[0]\n",
    "original_col = col_string\n",
    "# split column names out\n",
    "cols = col_string.split(';')\n",
    "# replace spaces with underscores\n",
    "for i in range(len(cols)):\n",
    "    cols[i] = cols[i].replace(' ', '_')\n",
    "    cols[i] = cols[i].replace('-', '_')\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# expand the single column into a dataframe\n",
    "pd_demographic_df = pd_demographic_df[original_col].str.split(';',expand=True)\n",
    "pd_demographic_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write demographics to csv in s3 storage\n",
    "pd_demographic_df = pd_demographic_df.drop_duplicates()\n",
    "write_to_staging(pd_demographic_df, 'qscapstone', 'demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data model for this project will conform to a star schema where the facts table is the immigration data which is connected to dimension tables for airports and demographics.  The airports and demographics tables are used to perform aggregation to see how many immigrants visited via different airport types and airports as well as aggregation on the states they visited and aggregation on the majority ethnicity in the cities they visited.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1) Clean data sets and create staging tables in s3 Bucket \"qscapstone\"\n",
    "    immigration.csv\n",
    "    airports.csv\n",
    "    demographics.csv\n",
    "    \n",
    "2) Upload cleaned data to s3 storage\n",
    "3) copy storage csv to staging tables in Amazon Redshift\n",
    "3) Create immigration_facts facts table\n",
    "3) Create airports_dimension table which joins to immigration_facts\n",
    "4) Create demographics_dimension table which joins to immigration_facts\n",
    "5) Perform aggregation on Airports\n",
    "6) Perform aggregation on Demographics \n",
    "7) Run tests to ensure data integrity/pipline success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def connect_to_reshift(host, dbname, user, password, port):\n",
    "    \"\"\"Function that returns a psycopg2 db connection\n",
    "    Args:\n",
    "        host (str): host\n",
    "        dbname (str): database\n",
    "        password (str): user password\n",
    "        port (int): \n",
    "    \"\"\"\n",
    "    connection_string = f'host={host} dbname={dbname} user={user} password={password} port={port}'\n",
    "    conn = psycopg2.connect(connection_string)\n",
    "    \n",
    "    return conn       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def drop_tables(conn, tables):\n",
    "    \"\"\"Function that drops all tables\n",
    "    Args:\n",
    "        conn (connection): psycopg2 connection\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            query = 'DROP TABLE ' + table\n",
    "            cur.execute(query)\n",
    "        except:\n",
    "            conn.rollback()\n",
    "        \n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tables = ['staging_airports', 'staging_demographics', 'staging_immigration']\n",
    "# drop_tables(conn, ['airports_dimension', 'demographics_dimension', 'immigration_facts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_tables(conn):\n",
    "    '''Function that creates tables in redshift\n",
    "    Args:\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # staging tables\n",
    "    create_staging_immigration = '''\n",
    "    CREATE TABLE IF NOT EXISTS staging_immigration(\n",
    "        im_id integer identity(0,1) PRIMARY KEY,\n",
    "        i94cit varchar, \n",
    "        i94port varchar NOT NULL, \n",
    "        i94mode varchar,\n",
    "        arrdate varchar,\n",
    "        i94res varchar,\n",
    "        i94addr varchar,\n",
    "        depDate varchar,\n",
    "        i94bir varchar,\n",
    "        i94visa varchar,\n",
    "        gender varchar\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    create_staging_airports = '''\n",
    "    CREATE TABLE IF NOT EXISTS staging_airports(\n",
    "        ident varchar PRIMARY KEY,\n",
    "        type varchar,\n",
    "        name varchar, \n",
    "        iso_region varchar, \n",
    "        state varchar,\n",
    "        municipality varchar, \n",
    "        iata_code varchar, \n",
    "        latitude float, \n",
    "        longitude float\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    create_staging_demographics = '''\n",
    "    CREATE TABLE IF NOT EXISTS staging_demographics(\n",
    "         city_id integer identity(0,1) PRIMARY KEY,\n",
    "         City varchar,\n",
    "         State varchar,\n",
    "         Median_Age real,\n",
    "         Male_Population integer,\n",
    "         Female_Population integer,\n",
    "         Total_Population integer,\n",
    "         Number_of_Veterans integer,\n",
    "         Foreign_born integer,\n",
    "         Average_Household_Size real,\n",
    "         State_Code varchar,\n",
    "         Race varchar,\n",
    "         Count integer,\n",
    "         UNIQUE(City, State)\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    # facts table\n",
    "    create_immigration_facts = '''\n",
    "    CREATE TABLE IF NOT EXISTS immigration_facts(\n",
    "        im_id integer PRIMARY KEY,\n",
    "        i94port varchar NOT NULL,\n",
    "        city_id integer NOT NULL,\n",
    "        i94cit varchar, \n",
    "        i94mode varchar,\n",
    "        arrdate varchar,\n",
    "        i94res varchar,\n",
    "        i94addr varchar,\n",
    "        depDate varchar,\n",
    "        i94bir varchar,\n",
    "        i94visa varchar,\n",
    "        gender varchar\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    # dimension tables\n",
    "    create_airports_dimension = '''\n",
    "    CREATE TABLE IF NOT EXISTS airports_dimension(\n",
    "        im_id integer NOT NULL PRIMARY KEY,\n",
    "        ident varchar NOT NULL,\n",
    "        type varchar,\n",
    "        name varchar, \n",
    "        iso_region varchar, \n",
    "        state varchar,\n",
    "        municipality varchar, \n",
    "        iata_code varchar, \n",
    "        latitude float, \n",
    "        longitude float\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    create_demographics_dimension = '''\n",
    "    CREATE TABLE IF NOT EXISTS demographics_dimension(\n",
    "         im_id integer NOT NULL PRIMARY KEY,\n",
    "         city_id integer NOT NULL,\n",
    "         City varchar,\n",
    "         State varchar,\n",
    "         Median_Age real,\n",
    "         Male_Population integer,\n",
    "         Female_Population integer,\n",
    "         Total_Population integer,\n",
    "         Number_of_Veterans integer,\n",
    "         Foreign_born integer,\n",
    "         Average_Household_Size real,\n",
    "         State_Code varchar,\n",
    "         Race varchar,\n",
    "         Count integer,\n",
    "         UNIQUE(City, State)\n",
    "    );\n",
    "    '''\n",
    "    \n",
    "    # aggregations\n",
    "    create_type_counts = '''\n",
    "    CREATE TABLE IF NOT EXISTS type_counts(\n",
    "        type varchar PRIMARY KEY,\n",
    "        visit_counts integer NOT NULL\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    create_airport_counts = '''\n",
    "    CREATE TABLE IF NOT EXISTS airport_counts(\n",
    "        name varchar PRIMARY KEY,\n",
    "        visit_counts integer NOT NULL\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    create_state_counts = '''\n",
    "    CREATE TABLE IF NOT EXISTS state_counts(\n",
    "        State_Code varchar PRIMARY KEY,\n",
    "        visit_counts integer NOT NULL\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    create_race_counts = '''\n",
    "    CREATE TABLE IF NOT EXISTS race_counts(\n",
    "        Race varchar PRIMARY KEY,\n",
    "        visit_counts integer NOT NULL\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    create_queries = [\n",
    "                      create_staging_immigration, \n",
    "                      create_staging_airports, \n",
    "                      create_staging_demographics,\n",
    "                      create_immigration_facts, \n",
    "                      create_airports_dimension, \n",
    "                      create_demographics_dimension,\n",
    "                      create_type_counts,\n",
    "                      create_airport_counts,\n",
    "                      create_state_counts,\n",
    "                      create_race_counts\n",
    "    ]\n",
    "    \n",
    "    for query in create_queries:\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "        except:\n",
    "            conn.rollback()\n",
    "            print('error in making table: ', query)\n",
    "            \n",
    "            \n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# conn = connect_to_reshift(HOST, DB, DB_USER, DB_PASSWORD, 5439)\n",
    "# create_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cur.execute('SElect * from staging_demographics')\n",
    "# result = cur.fetchone()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copy_staging(conn, bucket, csv_files, tables):\n",
    "    '''Function that copies csv files to staging tables\n",
    "    Args:\n",
    "        conn (connection): pyscopg2 connection\n",
    "        bucket (str): aws bucket\n",
    "        csv_files ([str]): list of csv_files\n",
    "        tables ([str]): corresponding tables for copying\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    staging_copy = \"\"\"\n",
    "    COPY {}\n",
    "    FROM '{}'\n",
    "    IAM_ROLE '{}'\n",
    "    REGION 'us-west-2'\n",
    "    CSV IGNOREHEADER 1;\n",
    "    \"\"\"\n",
    "    \n",
    "    cur =conn.cursor()\n",
    "                    \n",
    "    for i in range(len(csv_files)):\n",
    "        path = 's3://{}/{}'.format(bucket, csv_files[i])\n",
    "        try:\n",
    "            query = staging_copy.format(tables[i], path, ARN)\n",
    "            cur.execute(query)\n",
    "        except:\n",
    "            conn.rollback()\n",
    "            print('The following query failed: ', query)\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# conn = connect_to_reshift(HOST, DB, DB_USER, DB_PASSWORD, 5439)\n",
    "# copy_staging(conn, 'qscapstone', ['airports.csv', 'demographics.csv', 'immigration.csv'], \n",
    "#              ['staging_airports', 'staging_demographics', 'staging_immigration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def insert_facts_table(conn):\n",
    "    \"\"\"Function that insertst facts table\n",
    "    Args:\n",
    "        conn (conection): psycopg2 connection\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    query = '''\n",
    "    INSERT INTO immigration_facts\n",
    "    (SELECT \n",
    "        si.im_id,\n",
    "        si.i94port,\n",
    "        sd.city_id,\n",
    "        si.i94cit,\n",
    "        si.i94mode,\n",
    "        si.arrdate,\n",
    "        si.i94res,\n",
    "        si.i94addr,\n",
    "        si.depDate,\n",
    "        si.i94bir,\n",
    "        si.i94visa,\n",
    "        si.gender\n",
    "    FROM\n",
    "        staging_immigration AS si INNER JOIN \n",
    "        staging_airports AS sa ON si.i94port = sa.iata_code  AND sa.state = si.i94addr INNER JOIN\n",
    "        staging_demographics AS sd ON sa.municipality = sd.city and sa.state = sd.state_code);\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        cur.execute(query)\n",
    "    except:\n",
    "        conn.rollback()\n",
    "        \n",
    "    conn.commit()\n",
    "    cur.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# insert_facts_table(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def insert_dimensions_table(conn):\n",
    "    \"\"\"Function that dimensions facts table\n",
    "    Args:\n",
    "        conn (connection): psycopg2 connection\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    airports_query = '''\n",
    "    INSERT INTO airports_dimension \n",
    "    (\n",
    "    SELECT \n",
    "        imf.im_id,\n",
    "        sa.ident,\n",
    "        sa.type,\n",
    "        sa.name,\n",
    "        sa.iso_region,\n",
    "        sa.municipality,\n",
    "        sa.iata_code,\n",
    "        sa.latitude,\n",
    "        sa.longitude\n",
    "    FROM\n",
    "        immigration_facts AS imf INNER JOIN staging_airports AS sa\n",
    "        ON imf.i94port = sa.iata_code AND imf.i94addr = sa.state\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    demographics_query = '''\n",
    "    INSERT INTO demographics_dimension\n",
    "    (\n",
    "    SELECT\n",
    "        imf.im_id,\n",
    "        sd.city_id,\n",
    "        sd.City,\n",
    "        sd.State,\n",
    "        sd.Median_Age,\n",
    "        sd.Male_Population,\n",
    "        sd.Female_Population,\n",
    "        sd.Total_Population,\n",
    "        sd.Number_of_Veterans,\n",
    "        sd.Foreign_born,\n",
    "        sd.Average_Household_Size,\n",
    "        sd.State_Code,\n",
    "        sd.Race,\n",
    "        sd.Count\n",
    "    FROM\n",
    "        immigration_facts AS imf INNER JOIN staging_demographics AS sd\n",
    "        ON imf.city_id = sd.city_id AND imf.i94addr = sd.state_code\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    queries = [airports_query, demographics_query]\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    for query in queries:\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "        except Exception as e:\n",
    "            print('query failed: ', query)\n",
    "            print(e)\n",
    "            conn.rollback()\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perform_aggregation(conn):\n",
    "    \"\"\"Function that performs aggregation on dimensions tables\n",
    "    Args:\n",
    "        conn (connection): psycopg2 connection\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    group_type = '''\n",
    "    INSERT INTO type_counts\n",
    "    (\n",
    "    SELECT\n",
    "        type,\n",
    "        COUNT(*) as num_visits\n",
    "    FROM airports_dimension\n",
    "    GROUP BY type\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    group_name = '''\n",
    "    INSERT INTO airport_counts\n",
    "    (\n",
    "    SELECT\n",
    "        name,\n",
    "        COUNT(*) as num_visits\n",
    "    FROM airports_dimension\n",
    "    GROUP BY name\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    group_state = '''\n",
    "    INSERT INTO state_counts\n",
    "    (\n",
    "    SELECT\n",
    "        State_Code,\n",
    "        COUNT(*) as num_visits\n",
    "    FROM demographics_dimension\n",
    "    GROUP BY State_Code\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    group_race = '''\n",
    "    INSERT INTO race_counts\n",
    "    (\n",
    "    SELECT\n",
    "        Race,\n",
    "        COUNT(*) as num_visits\n",
    "    FROM demographics_dimension\n",
    "    GROUP BY Race\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    queries = [group_type, group_name, group_state, group_race]\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    for query in queries:\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "        except:\n",
    "            print('query failed: ', query)\n",
    "            conn.rollback()\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# perform_aggregation(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Pipeline():\n",
    "    '''Function to run a data pipeline\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # connect to Redshift\n",
    "    print('Connecting to Redshift...')\n",
    "    conn = connect_to_reshift(HOST, DB, DB_USER, DB_PASSWORD, 5439) \n",
    "    \n",
    "    # drop tables\n",
    "    tables = [\n",
    "        'staging_immigration',\n",
    "        'staging_airports',\n",
    "        'staging_demographics',\n",
    "        'immigration_facts',\n",
    "        'airports_dimension',\n",
    "        'demographics_dimension',\n",
    "        'type_counts', \n",
    "        'airport_counts', \n",
    "        'state_counts', \n",
    "        'race_counts'\n",
    "    ]\n",
    "    \n",
    "    print('Dropping Tables...')\n",
    "    drop_tables(conn, tables)\n",
    "    \n",
    "    print('Creating Tables...')\n",
    "    # create tables\n",
    "    create_tables(conn)\n",
    "    \n",
    "    print('Copying Staging Tables...')\n",
    "    # copy staging tables\n",
    "    copy_staging(conn, 'qscapstone', ['airports.csv', 'demographics.csv', 'immigration.csv'], \n",
    "             ['staging_airports', 'staging_demographics', 'staging_immigration'])\n",
    "    \n",
    "    print('Inserting Facts Table...')\n",
    "    # insert facts table\n",
    "    insert_facts_table(conn)\n",
    "    \n",
    "    print('Inserting Dimension Tables...')\n",
    "    # insert dimension tables\n",
    "    insert_dimensions_table(conn)\n",
    "    \n",
    "    print('Inserting Aggregate Tables...')\n",
    "    # create aggregation tables\n",
    "    perform_aggregation(conn)\n",
    "    \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redshift...\n",
      "Dropping Tables...\n",
      "Creating Tables...\n",
      "Copying Staging Tables...\n",
      "Inserting Facts Table...\n",
      "Inserting Dimension Tables...\n",
      "Inserting Aggregate Tables...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def check_data():\n",
    "    '''Function to perform ETL\n",
    "    Args:\n",
    "        None\n",
    "    Returns\n",
    "        (bool): success or failure\n",
    "    '''\n",
    "    conn = connect_to_reshift(HOST, DB, DB_USER, DB_PASSWORD, 5439) \n",
    "    \n",
    "    # check for rows in all tables\n",
    "    tables = [\n",
    "              'staging_airports', \n",
    "              'staging_demographics', \n",
    "              'staging_immigration',\n",
    "              'immigration_facts',\n",
    "              'airports_dimension',\n",
    "              'demographics_dimension',\n",
    "              'type_counts',\n",
    "              'airport_counts',\n",
    "              'state_counts',\n",
    "              'race_counts'\n",
    "             ]\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    for table in tables:\n",
    "        query = 'SELECT COUNT(*) FROM ' + table\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            result = cur.fetchone()\n",
    "            if(result[0] <= 0):\n",
    "                print('table is missing rows: ', table)\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print('query failed: ', query)\n",
    "            print(e)\n",
    "            conn.rollback()\n",
    "            \n",
    "    # make sure data isn't being repeated\n",
    "    airports_dimension = 'SELECT COUNT(*) FROM airports_dimension'\n",
    "    cur.execute(airports_dimension)\n",
    "    airports_counts = cur.fetchone()[0]\n",
    "    \n",
    "    airports_staging = 'SELECT count(*) FROM staging_airports'\n",
    "    cur.execute(airports_staging)\n",
    "    airports_staging_counts = cur.fetchone()[0]\n",
    "    \n",
    "    if airports_counts < airports_staging_counts:\n",
    "        print('Error in airport dimensions')\n",
    "        return False\n",
    "            \n",
    "    cur.close()\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "relation \"airports_staging\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-6846732094e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-84c4cbd05eac>\u001b[0m in \u001b[0;36mcheck_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mairports_staging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SELECT count(*) FROM airports_staging'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairports_staging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mairports_staging_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: relation \"airports_staging\" does not exist\n"
     ]
    }
   ],
   "source": [
    "check_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Table Wise Data Dictionary\n",
    "staging_immigration:\n",
    "        im_id integer - primary key identifying immigration row\n",
    "        i94cit varchar - city key\n",
    "        i94port varchar - port\n",
    "        i94mode varchar - i94mode\n",
    "        arrdate varchar - arrival date\n",
    "        i94res varchar - resident country code\n",
    "        i94addr varchar - resident address\n",
    "        depDate varchar - date\n",
    "        i94bir varchar - birth year\n",
    "        i94visa varchar - type of visa\n",
    "        gender varchar - gender\n",
    "   \n",
    "staging_airports:\n",
    "        ident varchar - primary key identifying airport\n",
    "        type varchar - type of airport\n",
    "        name varchar  - name of airport\n",
    "        iso_region varchar - region code\n",
    "        municipality varchar  - city\n",
    "        iata_code varchar - airport code, used to join to immigration\n",
    "        latitude float - lattitude\n",
    "        longitude float - longitude\n",
    "   \n",
    "staging_demographics:\n",
    "        city_id integer - primary key for cities\n",
    "        City varchar - city name\n",
    "        State varchar - state\n",
    "        Median_Age real - median age\n",
    "        Male_Population integer - numbe rof males\n",
    "        Female_Population integer - number of females\n",
    "        Total_Population integer - total population\n",
    "        Number_of_Veterans integer - number of veterans\n",
    "        Foreign_born integer - number of foreign born veterans\n",
    "        Average_Household_Size real - average household size\n",
    "        State_Code varchar - state code\n",
    "        Race varchar - majority race\n",
    "        Count integer - number of majority race\n",
    "    \n",
    "immigration_facts:\n",
    "        im_id integer - primary key identifying immigration row\n",
    "        i94cit varchar - city key\n",
    "        i94port varchar - port\n",
    "        city_id - id for city, joins to demographics\n",
    "        i94mode varchar - i94mode\n",
    "        arrdate varchar - arrival date\n",
    "        i94res varchar - resident country code\n",
    "        i94addr varchar - resident address\n",
    "        depDate varchar - date\n",
    "        i94bir varchar - birth year\n",
    "        i94visa varchar - type of visa\n",
    "        gender varchar - gender\n",
    "    \n",
    "airports_dimension:\n",
    "        im_id - immigration id, joins to immigration\n",
    "        ident varchar - primary key identifying airport\n",
    "        type varchar - type of airport\n",
    "        name varchar  - name of airport\n",
    "        iso_region varchar - region code\n",
    "        municipality varchar  - city\n",
    "        iata_code varchar - airport code, used to join to immigration\n",
    "        latitude float - lattitude\n",
    "        longitude float - longitude\n",
    "    \n",
    "demographics_dimension:\n",
    "        im_id - immigration id, joins to immigration\n",
    "        city_id integer - primary key for cities\n",
    "        City varchar - city name\n",
    "        State varchar - state\n",
    "        Median_Age real - median age\n",
    "        Male_Population integer - numbe rof males\n",
    "        Female_Population integer - number of females\n",
    "        Total_Population integer - total population\n",
    "        Number_of_Veterans integer - number of veterans\n",
    "        Foreign_born integer - number of foreign born veterans\n",
    "        Average_Household_Size real - average household size\n",
    "        State_Code varchar - state code\n",
    "        Race varchar - majority race\n",
    "        Count integer - number of majority race\n",
    "    \n",
    "type_counts:\n",
    "        type varchar - type of airport\n",
    "        visit_counts integer - total number of visits\n",
    "    \n",
    "airport_counts:\n",
    "        name varchar - airport name\n",
    "        visit_counts integer - total number of visits \n",
    "    \n",
    "state_counts:\n",
    "        State_Code varchar - state code\n",
    "        visit_counts integer - total number of visits \n",
    "    \n",
    "race_counts:\n",
    "        Race varchar - majority race in city\n",
    "        visit_counts integer - total number of visits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Project Description:\n",
    "In this project, data from four different data sets was cleaned, evaluated, and combined to make a database that can be used to asses immigration patterns in the united states.  In the first part of the project, each of these data sets were cleaned and then relevant data was uploaded as csv files to an s3 storage bucket to create a quick and efficient way to get the data into an amazon redshift database.  After uploading the data to s3 storage, a database was created in amazon Redshift.  Redshift was used to do its columnar storage which allows quick analytical quires to run.  The original csv data was inserted into staging tables.  Next, fact tables and dimmension tables were created by querying the staging tables.  After the Staging tables were made, aggregation analysis was run to examine the total number of immigrant visits relative to the type of airport, the airport name, the state, and the majority race of the city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Pipeline Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The immigration dataset for this project consists of a single month of air travel immigration data for the US.  Therefore, the pipeline should be run every month after the data sets have been collected.  Additional analytical queries can be run on the database in an adhoc manner whenever necessary in order to gain further insights into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### If The Data Sets Changed \n",
    "#### The data was increased by 100x.\n",
    "\n",
    "The s3 storage used should be sufficient to handle these data demmands.  The database would be fine because redshift is able to dynamically scale to handle the necessary workload.\n",
    "\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "In this case, we would want to use a data pipeline scheduler such as Airflow to schedule jobs to run at the end of each day.  Trial runs would need to be run to assess the timeframe of the pipeline.  Once a timeframe was established, SLAs could be used to ensure that the pipeline is running within the expected tie duration.  If the jobs ran past the SLA, emails could be sent to the DB admin to indicate that tere is a problem with the pipeline.\n",
    "\n",
    "#### The database needed to be accessed by 100+ people.\n",
    "Redshift should be able to scale and handle the load.  However, if it is not, Apache cassandra could be used instead.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
